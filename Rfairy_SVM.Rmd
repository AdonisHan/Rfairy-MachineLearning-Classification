---
title: "Support Vector Machine"
output: 
  html_notebook:
    toc: true
---

```{r libs, include=FALSE}
Sys.setlocale('LC_ALL', 'ko_KR.UTF-8')
```




9장에서, 우리는 1990년대 컴퓨터 과학 커뮤니티에서 개발되었고 이후 현재까지 인기가 상승하고 있는 분류를 위한 하나의 접근, support vector machine (SVM)을 논의한다. SVMs는 다양한 세팅에서 성능이 좋다고 증명되고 있고, 분류기중 최고의 격이 다른 (out of the box) 중의 하나로 빈번하게 고려된다. 

Support vector machine은 9.1절에서 소개되는 maximal margin classifier라 불리는 간단하고 직관적인 분류기의 일반화이다. Maximal margin classifier가 우아하고 간단하지만, 이 분류기는 안타깝게도 클래스들이 선형 경계(linear boundary)에 의해 분리될 수 있는 데이터만을 요구하기 때문에 대부분의 데이터에 적용될 수 없다. 9.2절에서 우리는 보다 넓은 범위에서 적용될 수 있는 maximal margin classifier의 확장인 support vector classifier를 소개한다. 9.3절은 비선형 클래스 경계를 수용하기 위한 support vector classifier를 좀 더 확장한 support vector machine을 소개한다. Support vector machine은 클래스가 두 개 있는 이항의 분류 문제를 위해 만들어졌다; 9.4절에서 클래스의 수가 2보다 많은 경우에 대한 support vector machine의 확장을 논의한다. 9.5절에서 우리는 support vector machine과 로지스틱 회귀와 같은 다른 통계적 기법 사이의 밀접한 관계를 논의한다. 

사람들은 종종 maximal margin classifier, support vector classifier, 그리고 support vector machine을 "support vector machines"로 엄밀하지 않게 말한다. 혼돈을 피하기 위해, 우리는 이 장에서 이 세 가지 개념들을 주의 깊게 구별할 것이다. 

이 섹션에서, 우리는 초평면(hyperplane)을 정의하고 최적 분할 초평면의 개념을 소개한다. 
9.1.1 What Is a Hyperplane?
pp-차원 공간에서, 초평면(hyperplane)은 차원 p−1p−1의 평평한 아핀(affine; 원점을 통과할 필요가 없는 부분공간을 나타냄) 부분공간이다. 예를 들어, 2차원에서, 1차원의 평평한 부분공간, 즉 하나의 선(line)이다. 3차원에서 초평면은 평평한 2차원의 부분공간, 즉 하나의 평면이다. p>3p>3 차원에서, 초평면을 시각화하기가 어려울 수도 있지만, (p−1)(p−1)-차원의 평평한 부분공간의 개념은 여전히 적용된다.  


# 패키지
```{r}

options(repos = "https://cran.rstudio.com" )
library(caret)
library(e1071)
```

# iris
```{r}
index.train <- createDataPartition(iris$Species, p=0.8, list = FALSE)
iris.train <- iris[index.train,]
iris.test <- iris[-index.train,]
```

## SVM 훈련
* Support Vectors: 마진을 결정하는 점들
* gamma: kernel에 사용되는 모수
```{r}
svm.iris <- svm(Species ~ ., data=iris.train)
summary(svm.iris)
```

## 예측/교차검증
```{r}
pred.iris <- predict(svm.iris, iris.test)
confusionMatrix(pred.iris, iris.test$Species)
```

# kyphosis
```{r}
library(rpart)
index.train <- createDataPartition(kyphosis$Kyphosis, p=0.8, list = FALSE)
kyphosis.train <- kyphosis[index.train,]
kyphosis.test <- kyphosis[-index.train,]
```

## SVM 훈련
```{r}
svm.kyphosis <- svm(Kyphosis ~ ., data=kyphosis.train)
summary(svm.kyphosis)
```

## 예측/교차검증
```{r}
pred.kyphosis <- predict(svm.kyphosis, kyphosis.test)
confusionMatrix(pred.kyphosis, kyphosis.test$Kyphosis)
```

## Tune (fix)
```{r}
# grid search
obj.fix <- tune(svm, Kyphosis ~ ., data = kyphosis, 
            ranges = list(gamma = 2^(-5:1), cost = 2^(0:4)),
            tunecontrol = tune.control(sampling = "fix")
            )
summary(obj.fix)
plot(obj.fix)
```

## Tune (bootstrap)
```{r}
# grid search
obj.bootstrap <- tune(svm, Kyphosis ~ ., data = kyphosis, 
            ranges = list(gamma = 2^(-5:1), cost = 2^(0:4)),
            tunecontrol = tune.control(sampling = "bootstrap")
            )
summary(obj.bootstrap)
plot(obj.bootstrap)
```

## Tuned Test
```{r}
pred_tuned <- predict(obj.bootstrap$best.model, kyphosis.test)
confusionMatrix(pred_tuned, kyphosis.test$Kyphosis)
```

# kernlab (spam data)

```{r}
library(kernlab)
data(spam)
```

## 훈련/테스트 데이터
```{r}
index.train <- createDataPartition(spam$type, p=0.8, list = FALSE)
spam.train <- spam[index.train,]
spam.test <- spam[-index.train,]
```

## SVM 훈련
```{r}
svm.spam <- svm(type ~ ., data=spam.train)
summary(svm.spam)
```

## 예측/교차검증
```{r}
pred.spam <- predict(svm.spam, spam.test)
confusionMatrix(pred.spam, spam.test$type)
```

## Tune (cross validation)
```{r}
# grid search
obj.cross <- tune(svm, type ~ ., data = spam, 
            ranges = list(gamma = c(1:3)/100, cost = c(1:5)*2), #?? 
            tunecontrol = tune.control(sampling = "cross", cross=5)
            )
summary(obj.cross)
plot(obj.cross)
```

## Tuned Test
```{r}
pred_tuned <- predict(obj.cross$best.model, spam.test)
confusionMatrix(pred_tuned, spam.test$type)
```
