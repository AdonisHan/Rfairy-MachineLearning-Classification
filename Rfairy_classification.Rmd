---
title: "분류"
output: 
  html_notebook:
    toc: true
---
# 왜 쓰냐??  도저히 이것의 데이터가 모수적이지 않을때. 기하학적으로 비모수적일때. 

# Clustering vs classification 의 차이는?? 
# 정답이 없기때문에 k-means로 분류 를 정함.
# 정답을 보면 knn을 써서 모형을 예측할 수 있는 것입니다. 
# knn 방식: 최근접 분류 알고리즘. 은 기하학적인 방법으로 가장 가까운적
# kmeans는 확률론 적으로. 
# 연역법 -> 귀납법 / 귀납법 -> 연역법.

# open with re-encording by UTF-8

```{r libs, include=FALSE}
Sys.setlocale('LC_ALL', 'ko_KR.UTF-8')
```

# k-NN

## class 패키지
```{r}
library(class)
```

## 데이터 준비
```{r}
iris

## 훈련 데이터와 테스트 데이터를 0.67, 0.33의 확률로 나누기 위한 샘플 생성
ind <- sample(1:nrow(iris), 100)

## 수치 데이터와 Label을 따로 정의
iris.training <- iris[ind, 1:4]
iris.test <- iris[-ind, 1:4]

iris.trainLabels <- iris[ind, 5]
iris.testLabels <- iris[-ind, 5]
```

## 훈련 데이터
```{r}
iris.training
iris.trainLabels
```

## 테스트 데이터
```{r}
iris.testLabels
```

## 훈련/예측 (k=1) - 패키지 없이

* 4차원 거리 함수
```{r}
distance4 <- function(x, y) {
  tmp <- sqrt(sum((x[1]-y[1])^2+(x[2]-y[2])^2+(x[3]-y[3])^2+(x[4]-y[4])^2))
  return (tmp)
  }
```

* 첫 번째 훈련 데이터와 원점 사이의 거리
```{r}
distance4(iris.training[1,], cbind(0,0,0,0))
```

* 첫 번째 훈련 데이터와 첫 번째 테스트 데이터 사이의 거리
```{r}
distance4(iris.training[1,], iris.test[1,])
```

* 1 Nearest Neighbor를 계산하는 함수
* 입력값
    + x: 훈련 데이터
    + y: x와 거리를 계산하여 분류할 특정 데이터
    + labels: 레이블
* 출력: y의 위치와 분류된 class
```{r}
nn1 <- function(x, y, labels) {
  df <- data.frame()
  for(i in 1:nrow(x)) {
    df_tmp <- data.frame(dist=distance4(x[i,], y), class=labels[i])
    df <- rbind(df, df_tmp)
  }
  return (cbind(y, Species=df[which.min(df$dist),]$class))
}
```
* 95 번째 테스트 데이터 분류
```{r}
nn1(iris.training, iris.training[95,], iris.trainLabels)
```


### Plot: 직접 계산한 예측값 (k=1)
```{r}
plot(iris_pred_k1$Sepal.Length, iris_pred_k1$Sepal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred_k1$Species)])
plot(iris_pred_k1$Petal.Length, iris_pred_k1$Petal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred_k1$Species)])
```

### 혼동행렬 (k=1에 대해 패키지 없이 계산)
```{r}
library(caret)
confusionMatrix(iris_pred_k1$Species, iris.testLabels)
```

* Accuracy: 전체에서 맞게 예측한 비율
* Sensitivity: 실제 분류 중 맞게 예측한 비율
* Specificity: 실제 분류가 아닌 것 중 맞게 예측한 비율
* Pos Pred Value: 긍정 예측 중 올바른 예측 비율 -> 정밀도
* Neg Pred Value: 부정 예측 중 올바른 예측 비율
* Prevalence: 전체에서 긍정 예측 비율


## 훈련/예측 (k=5) - 패키지 없이

* 5 Nearest Neighbor를 계산하는 함수
* 입력값
    + x: 훈련 데이터
    + y: x와 거리를 계산하여 분류할 특정 데이터
    + labels: 레이블
* 출력: y의 위치와 분류된 class
```{r}
nn5 <- function(x, y, labels) {
  df <- data.frame()
  for(i in 1:nrow(x)) {
    df_tmp <- data.frame(dist=distance4(x[i,], y), class=labels[i])
    df <- rbind(df, df_tmp)
  }
  dist.sort <- sort(df$dist, index.return=TRUE)
  n1 <- df[dist.sort$ix[1],]
  n2 <- df[dist.sort$ix[2],]
  n3 <- df[dist.sort$ix[3],]
  n4 <- df[dist.sort$ix[4],]
  n5 <- df[dist.sort$ix[5],]
  class <- tail(names(sort(table(rbind(n1, n2, n3, n4, n5)$class))), n=1)
  return (cbind(y, Species=class))
}
```

* 95 번째 테스트 데이터 분류
```{r}
nn5(iris.training, iris.training[95,], iris.trainLabels)
```

* iris.test에 있는 값들을 분류
```{r}
iris_pred_k5 <- data.frame()
for(i in 1:nrow(iris.test)) {
  iris_pred_k5 <- rbind(iris_pred_k5, nn5(iris.training, iris.test[i,], iris.trainLabels))
}
iris_pred_k5
```

### Plot: 직접 계산한 예측값 (k=5)
```{r}
plot(iris_pred_k5$Sepal.Length, iris_pred_k5$Sepal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred_k5$Species)])
plot(iris_pred_k5$Petal.Length, iris_pred_k5$Petal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred_k5$Species)])
```

### 혼동행렬 (k=5에 대해 패키지 없이 계산)
```{r}
confusionMatrix(iris_pred_k5$Species, iris.testLabels)
```


## 훈련/예측 (k=3, 패키지 사용)
```{r}
iris_pred <- knn(train=iris.training, test=iris.test, cl=iris.trainLabels, k=3)
iris_pred
```

### 혼동행렬 (k=3)
```{r}
confusionMatrix(iris_pred, iris.testLabels)
```

### Plot (k=3)
```{r}
plot(iris.test$Sepal.Length, iris.test$Sepal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred)])
plot(iris.test$Petal.Length, iris.test$Petal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred)])
```

## 훈련/예측 (k=50)
```{r}
iris_pred_50 <- knn(train=iris.training, test=iris.test, cl=iris.trainLabels, k=50)
iris_pred_50
```

### 혼동행렬 (k=50)
```{r}
confusionMatrix(iris_pred_50, iris.testLabels)
```

### Plot (k=50)
```{r}
plot(iris.test$Sepal.Length, iris.test$Sepal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred_50)])
plot(iris.test$Petal.Length, iris.test$Petal.Width, pch=21, bg=c("red","green","blue")[unclass(iris_pred_50)])
```

## 최적의 k 찾기

* 10-fold cross validation
```{r}
ctrl <- trainControl(method="cv", number = 10)
# cv= class validation 10 = k 수. 10이상은 분석하기 좋지 않다.
# 10 ford = 10개로 나눈다. 그 중에 하나를 가지고 예측을 한다음에
# 정확도를 구하거나 평균을 구하는 방식. 
# 어떤 accuracy 가 높은지를 구하는 것. 
iris.train.cl <- cbind(iris.trainLabels, iris.training)
grid <- expand.grid(k=c(1,3,5,7,10,15,20,30,40))
# grid 를 하나 할때, 3개 할때, 그래서 오래걸리는 것임.
# 사람이 하기 힘듬. 일일히 계산하기엔...
# 결과를 보면 k=10 일때가 가장 accuracy가 높았다. 
# 결과 :  10개를 정하는 게 좋다. 
# The final value used for the model was k = 10.
knnFit <- train(iris.trainLabels ~ ., data = iris.train.cl, method = "knn", trControl = ctrl, tuneGrid=grid)
knnFit
```

* Plot
```{r}
plot(knnFit)
```

# ########################
##### 나이브 베이즈 #####
# ########################

## e1071 패키지
```{r}
library(e1071)
```

## 스팸 이메일 분류 예시
## 수식이 간단하니 손으로 계산해보기! 
### 사전 확률
```{r}
# 스팸 확률
spam <- 0.5
# 햄 확률
ham <- 0.5 
```

### 조건부 확률
```{r}
# 스팸일 때 'hello'라는 단어가 들어갈 확률: 30%
hello_spam <- 0.3
# 스팸일 때 'world'라는 단어가 들어갈 확률: 2% P(world|Spam)
world_spam <- 0.02
# 햄일 때 'hello'라는 단어가 들어갈 확률: 20%
hello_ham <- 0.2
# 햄일 때 'world'라는 단어가 들어갈 확률: 1%
world_ham <- 0.01
```

### 사후 확률
```{r}
# 'hello'가 들어갈 때 스팸일 확률
spam_hello <- hello_spam / (hello_spam + hello_ham)
spam_hello
# 'world'가 들어갈 때 스팸일 확률
spam_world <- world_spam / (world_spam + world_ham)
spam_world
# 'hello'가 들어갈 때 햄일 확률
ham_hello <- hello_ham / (hello_spam + hello_ham)
ham_hello
# 'world'가 들어갈 때 햄일 확률
ham_world <- world_ham / (world_spam + world_ham)
ham_world
```

### 나이브 베이즈 분류
## 모든 값에서 다 곱해버려라. 
```{r}
# {hello, world} 단어 집합이 들어갈 때 스팸일 확률
spam_hello * spam_world
# {hello, world} 단어 집합이 들어갈 때 햄일 확률
ham_hello * ham_world
```

* 따라서 스팸


## iris 데이터(연속형) 분류

### 사전 확률
```{r}
# setosa 
setosa <- nrow(iris[iris$Species=='setosa',])/nrow(iris)
setosa
# versicolor
versicolor <- nrow(iris[iris$Species=='versicolor',])/nrow(iris)
versicolor
# virginica
virginica <- nrow(iris[iris$Species=='virginica',])/nrow(iris)
virginica
```

### 조건부 확률 (Sepal.Length에 대해서만)
Sepal.Length가 5인 경우 조건부 확률을 위해서 정규 분포를 가정하고 평균과 표준편차 계산
```{r}
# setosa의 경우 Sepal.Length의 평균과 표준편차
mean_sepal.length_setosa <- mean(iris$Sepal.Length[iris$Species=='setosa'])
sd_sepal.length_setosa <- sd(iris$Sepal.Length[iris$Species=='setosa'])
# versicolor의 경우 Sepal.Length의 평균과 표준편차
mean_sepal.length_versicolor <- mean(iris$Sepal.Length[iris$Species=='versicolor'])
sd_sepal.length_versicolor <- sd(iris$Sepal.Length[iris$Species=='versicolor'])
# virginica의 경우 Sepal.Length의 평균과 표준편차
mean_sepal.length_virginica <- mean(iris$Sepal.Length[iris$Species=='virginica'])
sd_sepal.length_virginica <- sd(iris$Sepal.Length[iris$Species=='virginica'])
# 꽃 종류별 Sepal.Length의 확률 밀도
## 꽃 받침의 길이가 5일때, 정규에서 얼마나
p_sepal.length_setosa <- dnorm(5, mean=mean_sepal.length_setosa, sd=sd_sepal.length_setosa)
p_sepal.length_setosa
p_sepal.length_versicolor <- dnorm(5, mean=mean_sepal.length_setosa, sd=sd_sepal.length_versicolor)
p_sepal.length_versicolor
p_sepal.length_virginica <- dnorm(5, mean=mean_sepal.length_setosa, sd=sd_sepal.length_virginica)
p_sepal.length_virginica
```

### 사후 확률
```{r}
# Sepal.Length이 주어졌을 떄 setosa일 확률
p_setosa_sepal.length <- p_sepal.length_setosa / (p_sepal.length_setosa + p_sepal.length_versicolor + p_sepal.length_virginica)
p_setosa_sepal.length
```

* 같은 방법으로 Sepal.Length이 주어졌을 떄 versicolor일 확률과 virginica일 확률 각각 계산 후 비교

### 나이브 베이즈 분류 (e1071 패키지 사용)
```{r}
model <- naiveBayes(Species ~ ., data = iris[ind,])
model
```

### 예측
```{r}
predict(model, iris[-ind,-5])
iris.testLabels
```

### 혼동행렬
```{r}
confusionMatrix(predict(model, iris[-ind,-5]), iris.testLabels)
```

### Leave One Out Cross Validation
```{r}
train_control <- trainControl(method="LOOCV")
model <- train(Species~., data=iris, trControl=train_control, method="nb")
print(model)
```

* usekernel: Kernel Density Estimation


### 10-fold Cross Validation
```{r}
train_control <- trainControl(method="cv", number=10)
model <- train(Species~., data=iris, trControl=train_control, method="nb")
print(model)
```

# 연습

## 손글씨 숫자 분류
데이터: [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer)

## 키와 몸무게에 따른 성별 분류
파일: 01_heights_weights_genders.csv
